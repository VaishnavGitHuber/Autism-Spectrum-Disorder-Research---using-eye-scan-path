# -*- coding: utf-8 -*-
"""Autism_Transformed_imagesII_accuracy_cnn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11q9DnJjHtQctDZmrdyZSugPLoOECrLDU

### Author: Vaishnav Krishna P
- vyshnavkrishnap2020@gamil.com

### 1. Mount Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""### 2. Import Libraries"""

import os
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import matthews_corrcoef
import seaborn as sns

"""### 3. Define Dataset Path"""

data_dir = "/content/drive/MyDrive/TransformedImages_NoSkeleton"

IMG_SIZE = 128
BATCH_SIZE = 32

"""### 4. Load Dataset Automatically from Folders"""

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir,
    labels='inferred',
    label_mode='binary',
    image_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    validation_split=0.2,
    subset="both",
    seed=123
)

train_dataset, val_dataset = dataset

"""### 5. Normalize Images"""

normalization_layer = layers.Rescaling(1./255)

train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))
val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))

"""### 6: Build Basic CNN Model"""

model = models.Sequential([
    layers.Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(64, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),

    layers.Conv2D(128, (3,3), activation='relu'),
    layers.MaxPooling2D(2,2),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(1, activation='sigmoid')   # Binary classification
])

"""### 7: Compile Model"""

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

"""### 8: Train Model"""

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=20
)

"""### 9: Evaluate Model"""

# Get predictions
y_true = []
y_pred = []

for images, labels in val_dataset:
    preds = model.predict(images)
    preds = (preds > 0.5).astype(int)

    y_true.extend(labels.numpy())
    y_pred.extend(preds.flatten())

y_true = np.array(y_true)
y_pred = np.array(y_pred)

# Metrics
accuracy = accuracy_score(y_true, y_pred)
precision = precision_score(y_true, y_pred)
recall = recall_score(y_true, y_pred)  # Sensitivity
f1 = f1_score(y_true, y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall (Sensitivity):", recall)
print("F1 Score:", f1)

mcc = matthews_corrcoef(y_true, y_pred)

print("Matthews Correlation Coefficient (MCC):", mcc)

"""### 10: Confusion Matrix"""

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(5,5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["No ASD", "ASD"],
            yticklabels=["No ASD", "ASD"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Get history
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(1, len(acc) + 1)

# Plot Accuracy
plt.figure()
plt.plot(epochs_range, acc, marker='o')
plt.plot(epochs_range, val_acc, marker='s')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy')
plt.legend(['Training Accuracy', 'Validation Accuracy'])
plt.show()

# Plot Loss
plt.figure()
plt.plot(epochs_range, loss, marker='o')
plt.plot(epochs_range, val_loss, marker='s')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training and Validation Loss')
plt.legend(['Training Loss', 'Validation Loss'])
plt.show()

"""- Note: The large discrepancy between training (99%) and validation (75%) accuracy confirms overfitting in the CNN model."""